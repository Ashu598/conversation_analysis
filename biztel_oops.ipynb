{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeIAkhwqfcLYU8y5iUXSME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashu598/conversation_analysis/blob/main/biztel_oops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement OOP Principles for Data Pipeline"
      ],
      "metadata": {
        "id": "sN4KQGj_LUHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVoKN9wILMNd",
        "outputId": "d9f1e82c-f50d-4b24-a2f4-2fe4fd25195a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          conversation_id  \\\n",
            "0  t_d004c097-424d-45d4-8f91-833d85c2da31   \n",
            "1  t_d004c097-424d-45d4-8f91-833d85c2da31   \n",
            "2  t_d004c097-424d-45d4-8f91-833d85c2da31   \n",
            "3  t_d004c097-424d-45d4-8f91-833d85c2da31   \n",
            "4  t_d004c097-424d-45d4-8f91-833d85c2da31   \n",
            "\n",
            "                                         article_url  config  \\\n",
            "0  https://www.washingtonpost.com/sports/colleges...       3   \n",
            "1  https://www.washingtonpost.com/sports/colleges...       3   \n",
            "2  https://www.washingtonpost.com/sports/colleges...       3   \n",
            "3  https://www.washingtonpost.com/sports/colleges...       3   \n",
            "4  https://www.washingtonpost.com/sports/colleges...       3   \n",
            "\n",
            "                                             message  agent  sentiment  \\\n",
            "0  Did you know that the University of Iowa's loc...      1        NaN   \n",
            "1  I think I did hear something about that.  I im...      2        4.0   \n",
            "2  So, it would be in the visiting team's locker ...      1        NaN   \n",
            "3  Right.  Teams do all kinds of things to bother...      2        4.0   \n",
            "4  I would hate a cold bench. Then again, I would...      1        4.0   \n",
            "\n",
            "     knowledge_source  turn_rating  agent_1_rating  agent_2_rating  \n",
            "0                 FS1          3.0             3.0               3  \n",
            "1                 FS1          3.0             3.0               3  \n",
            "2                 FS1          3.0             3.0               3  \n",
            "3                 FS1          3.0             3.0               3  \n",
            "4  Personal Knowledge          3.0             3.0               3  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from math import pi\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "class DataPipeline:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.df = None\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        rows = []\n",
        "        for conv_id, conv_data in data.items():\n",
        "            article_url = conv_data.get(\"article_url\", \"\")\n",
        "            config = conv_data.get(\"config\", \"\")\n",
        "            conversation_rating = conv_data.get(\"conversation_rating\", {})\n",
        "\n",
        "            for turn in conv_data.get(\"content\", []):\n",
        "                rows.append({\n",
        "                    \"conversation_id\": conv_id,\n",
        "                    \"article_url\": article_url,\n",
        "                    \"config\": config,\n",
        "                    \"message\": turn[\"message\"],\n",
        "                    \"agent\": turn[\"agent\"],\n",
        "                    \"sentiment\": turn[\"sentiment\"],\n",
        "                    \"knowledge_source\": \", \".join(turn[\"knowledge_source\"]),\n",
        "                    \"turn_rating\": turn[\"turn_rating\"],\n",
        "                    \"agent_1_rating\": conversation_rating.get(\"agent_1\", \"\"),\n",
        "                    \"agent_2_rating\": conversation_rating.get(\"agent_2\", \"\")\n",
        "                })\n",
        "\n",
        "        self.df = pd.DataFrame(rows)\n",
        "        return self.df\n",
        "\n",
        "    # Convert Categorical Variables into Numerical Representations\n",
        "    def convert_numerical_columns(self):\n",
        "      df['config'] = df['config'].map({\"A\":1, \"B\":2, \"C\":3, \"D\":4})\n",
        "      df['agent'] = df['agent'].map({\"agent_1\":1, \"agent_2\":2})\n",
        "      df['sentiment'] = df['sentiment'].map({\"Fearful\":0, \"Angry\":1, \"Disgusted\":2, \"Sad\":3, \"Neutral\":4, \"Surprised\": 5, \"Happy\":6})\n",
        "      df['turn_rating'] = df['turn_rating'].map({\"Poor\":0, \"Not Good\":1, \"Passable\":2, \"Good\":3, \"Excellent\":4})\n",
        "      df['agent_1_rating'] = df['agent_1_rating'].map({\"Not Good\":1, \"Passable\":2, \"Good\":3, \"Excellent\":4})\n",
        "      df['agent_2_rating'] = df['agent_2_rating'].map({\"Not Good\":1, \"Passable\":2, \"Good\":3, \"Excellent\":4})\n",
        "      return self.df\n",
        "\n",
        "\n",
        "# Initialize Pipeline\n",
        "pipeline = DataPipeline(\"/content/BiztelAI_DS_Dataset_Mar'25.json\")\n",
        "df = pipeline.load_data()\n",
        "df = pipeline.convert_numerical_columns()\n",
        "#df[\"cleaned_message\"] = df[\"message\"].apply(preprocess_text)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()  # Convert to lowercase\n",
        "  text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Remove special characters\n",
        "  words = word_tokenize(text)  # Tokenization\n",
        "  words = [word for word in words if word not in stopwords.words(\"english\")]  # Remove stopwords\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
        "  return \" \".join(words)\n",
        "\n",
        "df[\"cleaned_message\"] = df[\"message\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "udG_o7hLMoNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f709O9DUMRqK",
        "outputId": "5ccac868-5427-4247-9522-e7d3ff458b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11760 entries, 0 to 11759\n",
            "Data columns (total 11 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   conversation_id   11760 non-null  object \n",
            " 1   article_url       11760 non-null  object \n",
            " 2   config            11760 non-null  int64  \n",
            " 3   message           11760 non-null  object \n",
            " 4   agent             11760 non-null  int64  \n",
            " 5   sentiment         6218 non-null   float64\n",
            " 6   knowledge_source  11760 non-null  object \n",
            " 7   turn_rating       11648 non-null  float64\n",
            " 8   agent_1_rating    11739 non-null  float64\n",
            " 9   agent_2_rating    11760 non-null  int64  \n",
            " 10  cleaned_message   11760 non-null  object \n",
            "dtypes: float64(3), int64(3), object(5)\n",
            "memory usage: 1010.8+ KB\n"
          ]
        }
      ]
    }
  ]
}